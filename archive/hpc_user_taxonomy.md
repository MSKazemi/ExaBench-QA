


---

## üß† HPC User Role Taxonomy (Comprehensive Overview)

Below is a **structured table** and a **detailed breakdown** for each role, describing:

1. **Primary Responsibilities / Tasks**
2. **Knowledge & Skill Areas (what they study or must understand)**
3. **Typical Data or Tools They Interact With**
4. **Key Query or Requirement Themes (for your RAG agent / chatbot use)**


| **Category / Role**                                                       | **Main Tasks & Responsibilities**                                                                                                                                                                                                            | **What They Study / Need to Know**                                                                                                                                                                                                       | **Tools / Data Interacted With**                                                                              | **Example Query Categories**                                                                                                                                                                             |
| ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| üßë‚Äçüíª **Normal HPC Users (Application Users / Scientists / Engineers)**   | - Submit and monitor jobs<br>- Manage input/output data<br>- Request compute/storage quotas<br>- Use preinstalled scientific software<br>- Debug failed jobs<br>- Optimize scripts for runtime efficiency                                    | - Linux shell, SLURM/Job schedulers<br>- Basic parallel computing concepts<br>- File systems (Lustre, GPFS)<br>- Module systems<br>- Data transfer (scp, rsync, Globus)                                                                  | Job scheduler logs, output/error files, user quota, environment modules, project directories                  | - ‚ÄúWhy did my job fail?‚Äù<br>- ‚ÄúHow to request more GPU nodes?‚Äù<br>- ‚ÄúWhich queue has shortest wait?‚Äù<br>- ‚ÄúHow to use Python with MPI?‚Äù                                                                  |
| üß™ **HPC Researchers (Computational Scientists / Performance Analysts)**  | - Develop new algorithms or parallel applications<br>- Profile, benchmark, and optimize applications<br>- Analyze performance and scalability<br>- Study power/energy efficiency<br>- Publish results                                        | - Advanced HPC programming (MPI, OpenMP, CUDA)<br>- Performance modeling (Roofline, scaling laws)<br>- Data analysis & profiling tools<br>- Energy efficiency & scheduling<br>- ML/AI for performance prediction                         | Profiling data (Perf, VTune, Arm MAP), benchmark results, telemetry (CPU/GPU metrics), cluster configurations | - ‚ÄúWhich configuration gives best energy-performance tradeoff?‚Äù<br>- ‚ÄúHow to interpret MPI imbalance metrics?‚Äù<br>- ‚ÄúCompare performance across nodes.‚Äù<br>- ‚ÄúHow to integrate Kepler power metrics?‚Äù    |
| üß∞ **System Administrators (SysAdmins / DevOps)**                         | - Install, configure, and maintain cluster OS and middleware<br>- Manage user accounts and security<br>- Monitor resource utilization and job queues<br>- Patch and upgrade software<br>- Troubleshoot system failures                       | - Linux administration, network security<br>- Scheduler configuration (SLURM, PBS)<br>- Containerization (Singularity, Podman)<br>- Automation (Ansible, Terraform, Helm)<br>- Logging & observability stacks (Prometheus, Grafana, ELK) | System logs, node health data, configuration files, container images, CI/CD pipelines                         | - ‚ÄúWhy is node n45 offline?‚Äù<br>- ‚ÄúUpdate CUDA driver on GPU partition.‚Äù<br>- ‚ÄúSet fair-share limits for projects.‚Äù<br>- ‚ÄúCheck SLURM daemon health.‚Äù                                                    |
| üè≠ **Facility Admin / Technicians (Datacenter Ops / Facility Engineers)** | - Maintain physical infrastructure: racks, cooling, power, networking<br>- Monitor environmental parameters (temperature, humidity)<br>- Respond to hardware alarms<br>- Coordinate maintenance windows<br>- Support sustainability goals    | - Electrical and mechanical systems<br>- BMS / DCIM systems<br>- IPMI, Redfish, SNMP monitoring<br>- Energy management and PUE<br>- Safety and compliance standards                                                                      | IPMI sensor data, BMS telemetry, facility dashboards, maintenance schedules                                   | - ‚ÄúWhich racks show thermal anomalies?‚Äù<br>- ‚ÄúCurrent PUE trend for datacenter?‚Äù<br>- ‚ÄúAlarm history for CRAC unit 3.‚Äù<br>- ‚ÄúCooling efficiency comparison between zones.‚Äù                               |
| üß± **HPC System Designers / Architects**                                  | - Design new clusters and storage systems<br>- Plan hardware and software architecture<br>- Evaluate technology options (CPU/GPU/Network)<br>- Define scalability and redundancy strategies<br>- Collaborate with vendors and research teams | - HPC architecture principles<br>- Network topology (Infiniband, Slingshot)<br>- Storage architecture (Lustre, BeeGFS)<br>- Power & cooling design<br>- Cost-performance modeling<br>- Benchmarking methodologies                        | System specs, RFPs, design models, simulation results, capacity planning data                                 | - ‚ÄúWhat is the optimal node design for AI + CFD?‚Äù<br>- ‚ÄúCompare cost of AMD vs Intel nodes per TFLOP.‚Äù<br>- ‚ÄúEstimate cooling load for 1 MW rack density.‚Äù<br>- ‚ÄúEvaluate interconnect latency impacts.‚Äù |

---

## üîç Deep Dive by Role

### 1. **Normal HPC Users**

* **Main Studies**:

  * Linux basics, shell scripting
  * Batch scheduling systems (SLURM, PBS, LSF)
  * Data management, file systems
  * Basic parallel execution (MPI runs, OpenMP threads)
* **Key Learning Topics**: job efficiency, resource requests, error diagnostics
* **Focus of Queries**: usability, quick help, job debugging, module usage

---

### 2. **HPC Researchers**

* **Main Studies**:

  * HPC programming models (MPI, OpenMP, CUDA, SYCL)
  * Profiling & performance analysis (TAU, Score-P, HPCToolkit)
  * Energy efficiency research (Kepler, RAPL, PowerAPI)
  * AI/ML-based performance modeling
  * Parallel algorithms & scalability theory
* **Focus of Queries**: optimization, reproducibility, automation, energy modeling

---

### 3. **System Administrators**

* **Main Studies**:

  * Cluster provisioning & automation (Ansible, Terraform, Helm)
  * Security (RBAC, LDAP, key management)
  * Logging, monitoring & observability (Prometheus, Grafana, Loki)
  * Scheduler management (SLURM config tuning)
  * Networking (IPMI, VLANs, InfiniBand)
* **Focus of Queries**: system health, deployment, upgrades, troubleshooting

---

### 4. **Facility Admin / Technicians**

* **Main Studies**:

  * Electrical power systems, UPS, PDU design
  * Cooling technologies (air/water, liquid immersion)
  * Environmental monitoring (IPMI, Modbus, SNMP)
  * Facility management software (BMS, DCIM)
  * Maintenance procedures, alarm systems
* **Focus of Queries**: environmental health, energy efficiency, maintenance logs

---

### 5. **HPC System Designers / Architects**

* **Main Studies**:

  * Cluster design methodologies
  * Hardware selection, cost modeling, procurement
  * Scalability simulations and workload characterization
  * System software stack planning (OS, scheduler, middleware)
  * Future tech trends (RISC-V, CXL, liquid cooling)
* **Focus of Queries**: architecture tradeoffs, design simulation, cost/performance analysis

---

## üí° Optional Add-on Categories (for completeness)

You could later extend taxonomy to include:

* **HPC Software Developers** (library/tool maintainers)
* **Data Scientists** using HPC-as-a-Service
* **Energy Managers** (cross between facility + sustainability)
* **AI/LLM Orchestrators** (future autonomous HPC optimization agents)


Excellent point ‚Äî you‚Äôre absolutely right ‚úÖ

Modern **HPC Researchers** (especially in Exascale-era systems and EU projects like DECICE/SEANERGYS) are not only doing *application-level* performance research, but also **operational data analytics**, **anomaly detection**, and **energy optimization** using full-stack telemetry.

Let‚Äôs update and **expand** the ‚ÄúHPC Researchers‚Äù category to fully reflect that ‚Äî integrating your additions into a structured breakdown suitable for taxonomy, RAG categories, and documentation.

---

## üß™ HPC Researchers (Updated & Complete)

| **Aspect**                                      | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ----------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Primary Mission**                             | Bridge scientific computing with data-driven optimization ‚Äî analyze, model, and improve performance, energy efficiency, reliability, and sustainability across the entire HPC system stack.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **Core Tasks & Responsibilities**               | - Develop new parallel algorithms or machine learning models for HPC workloads.<br>- **Perform anomaly detection** on multi-year cluster monitoring datasets (thermal, power, performance, logs).<br>- Analyze **system logs, scheduler traces, and job metadata** to identify performance bottlenecks and failure patterns.<br>- Conduct **power consumption estimation** from job scripts and runtime telemetry (batch script ‚Üí estimated Joules).<br>- Study **long-term facility & IT data** (cooling, temperature, power, humidity) to model correlations between compute load and energy usage.<br>- Perform **operational data analysis** for optimization and sustainability studies.<br>- Benchmark and profile scientific applications across architectures (CPU/GPU/FPGA).<br>- Develop predictive and prescriptive models for resource utilization, reliability, and cooling efficiency.<br>- Publish research results and propose novel HPC scheduling/energy-aware algorithms. |
| **Key Studies / Knowledge Areas**               | - **HPC performance engineering** (MPI, OpenMP, CUDA, SYCL)<br>- **Anomaly detection & time-series analysis** (ML/DL for logs & telemetry)<br>- **Operational data analytics** (Grafana/Prometheus data export, PostgreSQL, InfluxDB, parquet data)<br>- **Energy modeling & prediction** (Kepler, RAPL, PowerAPI, pod-level prediction models)<br>- **Cooling systems & facility telemetry interpretation** (BMS/DCIM data integration)<br>- **Data wrangling and visualization** (Pandas, PySpark, DVC, MLflow)<br>- **Scientific reproducibility** (experiment tracking, model versioning, FAIR datasets)<br>- **HPC workload characterization** (job patterns, resource usage clusters, performance variability)                                                                                                                                                                                                                                                                         |
| **Data Sources They Analyze**                   | - **Monitoring data:** node-level metrics (CPU/GPU utilization, power, temperature, fan speed)<br>- **Logs:** job scheduler logs, syslogs, application logs<br>- **Batch scripts:** SLURM/PBS job submission scripts (for workload classification & power estimation)<br>- **Databases:** Prometheus, PostgreSQL, InfluxDB, parquet archives<br>- **Facility telemetry:** PUE, cooling power, rack temperature distributions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **Tools & Frameworks**                          | - HPC profiling tools (HPCToolkit, TAU, Score-P, VTune)<br>- Data analytics frameworks (Pandas, Dask, PySpark, SQL)<br>- Visualization (Grafana, Plotly, Matplotlib)<br>- ML/DL frameworks (Scikit-Learn, PyTorch, TensorFlow)<br>- Energy monitoring (Kepler, RAPL, IPMI)<br>- Cluster telemetry ingestion (Prometheus, ExaData, InfluxDB)<br>- Reproducibility stacks (MLflow, DVC, GitLab CI/CD)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| **Research Themes / Study Topics**              | - System-wide **anomaly detection** (thermal, power, network)<br>- **Predictive maintenance** from multi-year logs<br>- **Energy-aware job scheduling and node management**<br>- **Workload characterization** and job class detection<br>- **Cross-correlation of IT + facility data** (server vs cooling)<br>- **Data-driven optimization of cooling & compute efficiency**<br>- **Multi-modal telemetry fusion** (metrics + logs + job scripts)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Example Queries for Chat Agent / RAG System** | - ‚ÄúDetect anomalies in node power consumption over the last 3 years.‚Äù<br>- ‚ÄúEstimate power cost of this batch script.‚Äù<br>- ‚ÄúWhich jobs caused thermal stress in 2023?‚Äù<br>- ‚ÄúCorrelate cooling energy and compute load during summer 2024.‚Äù<br>- ‚ÄúShow temporal drift in node utilization distribution.‚Äù<br>- ‚ÄúGenerate feature dataset for model training from Prometheus + SLURM logs.‚Äù                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

---

### üß© Integration Note (for your overall table)

This version should **replace** or **expand** the HPC Researcher section in your previous taxonomy table.

If you like, I can merge this updated version back into the **full 5-role Markdown table (`hpc_user_roles_full.md`)** with this expanded research role, including explicit sub-categories such as:

* *Performance Researchers*
* *Energy & Anomaly Researchers*
* *Facility Data Scientists*

Would you like me to regenerate the full Markdown file including this enhanced HPC Researcher section?



# üèóÔ∏è HPC & Monitoring System User Taxonomy

This document defines a **complete taxonomy of all user types** that interact with an **HPC system** and its **monitoring ecosystem** (IT + facility).  
It includes scientific users, operations teams, facility managers, researchers, and emerging AI-driven agents ‚Äî suitable for use in RAG models, chat systems, or knowledge graphs such as **ExaAgent**.

---

## üßë‚Äçüî¨ 1. Scientific & Application Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Normal HPC Users (Scientists / Engineers)** | Submit and monitor jobs, manage data, use pre-installed scientific software. | SLURM/PBS, job logs, project storage, modules, Singularity containers. | ‚ÄúWhy did my job fail?‚Äù<br>‚ÄúHow to request more GPU nodes?‚Äù |
| **HPC Researchers (Performance, Energy, Anomaly Studies)** | Analyze performance and energy efficiency; perform anomaly detection and long-term monitoring studies; estimate power from job scripts; optimize workloads and scheduling. | Monitoring DBs, Prometheus, InfluxDB, Kepler, logs, SLURM traces, facility data, DVC datasets. | ‚ÄúDetect anomalies in 3-year power data.‚Äù<br>‚ÄúEstimate energy of this batch script.‚Äù |
| **Data Scientists / AI Researchers on HPC** | Use HPC for ML/AI workloads; study scalability and data handling; integrate with model registries. | PyTorch, TensorFlow, MLflow, DVC, JupyterHub. | ‚ÄúTrain transformer model on GPU nodes.‚Äù |
| **Computational Domain Scientists** | Domain-specific workloads (CFD, MD, weather, etc.) using tuned software. | OpenFOAM, GROMACS, VASP, WRF, LAMMPS. | ‚ÄúHow to enable OpenMP in VASP job?‚Äù |
| **Industrial / Enterprise Users** | External customers accessing HPC through portals or APIs. | Web portals, REST APIs, workflow managers. | ‚ÄúSubmit workflow through API.‚Äù |

---

## üß∞ 2. Infrastructure & Operations Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **System Administrators (SysAdmins / DevOps)** | Maintain OS, scheduler, user accounts, and reliability of nodes. | Linux, SLURM, Prometheus, Ansible, Helm, Kubernetes, logs. | ‚ÄúWhy is node n45 offline?‚Äù |
| **Monitoring Engineers / Observability Specialists** | Manage metric collection, alerting, and dashboarding. | Prometheus, Grafana, Loki, Alertmanager, Kepler. | ‚ÄúCPU utilization trend per partition.‚Äù |
| **Storage Administrators** | Manage parallel file systems and backups. | Lustre, BeeGFS, Ceph, GPFS, object storage telemetry. | ‚ÄúCheck Lustre throughput anomalies.‚Äù |
| **Network Administrators** | Monitor and maintain interconnects. | SNMP, Infiniband, Slingshot, Grafana dashboards. | ‚ÄúLatency statistics for fabric ports.‚Äù |
| **Security Officers / IAM Managers** | Oversee authentication, RBAC, and compliance. | Keycloak, LDAP, IAM logs, audit trails. | ‚ÄúList failed login attempts.‚Äù |
| **Support Engineers / Helpdesk Staff** | Resolve user issues, collect logs, maintain FAQs. | Ticketing systems, job reports. | ‚ÄúUser job exceeded time limit.‚Äù |
| **Software Stack Maintainers** | Build and containerize HPC toolchains. | Spack, EasyBuild, CI/CD pipelines. | ‚ÄúRebuild GCC 13 toolchain with CUDA.‚Äù |

---

## üè≠ 3. Facility & Hardware Operations Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Facility Administrators / Technicians** | Manage racks, cooling, power, sensors, and safety. | IPMI, Redfish, BMS/DCIM, Modbus, SNMP. | ‚ÄúReport temperature map per rack.‚Äù |
| **Energy Managers / Sustainability Analysts** | Analyze PUE, energy efficiency, and sustainability. | Power meters, facility telemetry, PUE logs. | ‚ÄúCompute PUE for past quarter.‚Äù |
| **Hardware Engineers / Maintenance Staff** | Diagnose and replace faulty components. | IPMI logs, service tickets. | ‚ÄúWhich nodes reported fan failure?‚Äù |
| **Procurement & Vendor Liaisons** | Manage warranties and evaluate new hardware. | RFPs, inventory DBs. | ‚ÄúTrack warranty status for GPU nodes.‚Äù |

---

## üß± 4. Architecture, Design & Strategic Planning Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **HPC System Architects / Designers** | Design and size new clusters; define topology and storage. | RFPs, simulation models, benchmark data. | ‚ÄúEvaluate AMD vs Intel node cost/TFlop.‚Äù |
| **Capacity Planners** | Forecast usage, hardware refreshes, and expansion needs. | Historical metrics, queue statistics. | ‚ÄúPredict CPU core demand in 2026.‚Äù |
| **Performance Modelers** | Build performance and cost trade-off models. | Telemetry archives, simulators. | ‚ÄúSimulate scalability for 1k-node workload.‚Äù |
| **Policy & Project Managers** | Plan budgets, define priorities, align R&D goals. | KPI dashboards, utilization reports. | ‚ÄúMonthly utilization summary by project.‚Äù |

---

## üìä 5. Data Analytics & Observability Researchers (Cross-Layer)

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Operational Data Scientists** | Analyze long-term telemetry for optimization and sustainability. | SQL, Pandas, PySpark, parquet datasets. | ‚ÄúCorrelate cooling power and CPU load.‚Äù |
| **Anomaly Detection Researchers** | Use AI/ML to detect abnormal thermal/power/network patterns. | Kepler, ThermADNet, HazardNet, Grafana alerts. | ‚ÄúFind nodes with thermal drift.‚Äù |
| **Digital Twin Developers** | Create virtual replicas of HPC behavior for predictive control. | InfluxDB, ML models, simulation tools. | ‚ÄúTrain digital twin on 2022-2025 telemetry.‚Äù |
| **AIOps / MLOps Engineers** | Build autonomous pipelines for anomaly detection and optimization. | MLflow, DVC, FastAPI, LangGraph, Prometheus APIs. | ‚ÄúDeploy model to optimize node power caps.‚Äù |

---

## üß© 6. Management, Policy, and External Stakeholders

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Center Directors / Project PIs** | Oversee operations, KPIs, and collaborations. | KPI dashboards, energy reports. | ‚ÄúShow overall energy savings since 2023.‚Äù |
| **Funding Agencies / Auditors** | Evaluate efficiency, utilization, and sustainability. | Reports, compliance datasets. | ‚ÄúGenerate annual energy audit report.‚Äù |
| **External Collaborators (EU Partners / Vendors)** | Access shared telemetry or datasets for research. | Shared dashboards, APIs, data exports. | ‚ÄúRequest metrics subset for WP3 analysis.‚Äù |

---

## ü§ñ 7. Automation & Agentic Components (Emerging Virtual Users)

In AIOps-enabled HPC frameworks (e.g., **ExaAgent**, **KubeIntellect**), automated agents act as *virtual users* interacting with monitoring and control layers.

| **Agent Type** | **Purpose / Function** | **Interacts With** |
|----------------|------------------------|--------------------|
| **Monitoring Agent (ExaSage / Kepler)** | Collects metrics and logs across nodes and facilities. | Prometheus, IPMI, InfluxDB. |
| **Anomaly Detection Agent (ThermADNet / HazardNet)** | Detects and explains abnormal system behavior. | Telemetry DBs, alert systems. |
| **AI Scheduler Agent** | Dynamically optimizes job scheduling and resource allocation. | SLURM APIs, Prometheus metrics. |
| **Chat / RAG Agent (User Assistant)** | Provides conversational access to data, documentation, and monitoring systems. | Vector DB, documentation corpus, APIs. |

---

## üåê 8. Hierarchical Summary (Mind-Map)

```plaintext
HPC & Monitoring Users
‚îú‚îÄ‚îÄ Scientific Users
‚îÇ ‚îú‚îÄ‚îÄ Normal Users
‚îÇ ‚îú‚îÄ‚îÄ HPC Researchers
‚îÇ ‚îú‚îÄ‚îÄ AI/ML Researchers
‚îÇ ‚îú‚îÄ‚îÄ Domain Scientists
‚îÇ ‚îî‚îÄ‚îÄ Industrial Users
‚îú‚îÄ‚îÄ Infrastructure & Operations
‚îÇ ‚îú‚îÄ‚îÄ SysAdmins
‚îÇ ‚îú‚îÄ‚îÄ Monitoring Engineers
‚îÇ ‚îú‚îÄ‚îÄ Storage / Network Admins
‚îÇ ‚îú‚îÄ‚îÄ Security Officers
‚îÇ ‚îî‚îÄ‚îÄ Support Staff
‚îú‚îÄ‚îÄ Facility & Hardware
‚îÇ ‚îú‚îÄ‚îÄ Facility Technicians
‚îÇ ‚îú‚îÄ‚îÄ Energy Managers
‚îÇ ‚îú‚îÄ‚îÄ Hardware Engineers
‚îÇ ‚îî‚îÄ‚îÄ Vendors / Procurement
‚îú‚îÄ‚îÄ Architecture & Planning
‚îÇ ‚îú‚îÄ‚îÄ System Architects
‚îÇ ‚îú‚îÄ‚îÄ Capacity Planners
‚îÇ ‚îú‚îÄ‚îÄ Policy Managers
‚îÇ ‚îî‚îÄ‚îÄ Project Directors
‚îú‚îÄ‚îÄ Data Analytics & Research
‚îÇ ‚îú‚îÄ‚îÄ Operational Data Scientists
‚îÇ ‚îú‚îÄ‚îÄ Anomaly Detection Researchers
‚îÇ ‚îú‚îÄ‚îÄ Digital Twin Developers
‚îÇ ‚îî‚îÄ‚îÄ AIOps Engineers
‚îú‚îÄ‚îÄ Management & External
‚îÇ ‚îú‚îÄ‚îÄ Center Directors
‚îÇ ‚îú‚îÄ‚îÄ Funding Agencies
‚îÇ ‚îî‚îÄ‚îÄ External Collaborators
‚îî‚îÄ‚îÄ Automation Agents
‚îú‚îÄ‚îÄ Monitoring Agent
‚îú‚îÄ‚îÄ Anomaly Detection Agent
‚îú‚îÄ‚îÄ AI Scheduler Agent
‚îî‚îÄ‚îÄ Chat / RAG Agent
```


---

## üß© Notes for RAG / Chat Integration

- Each **user type** can be mapped to:
  - `knowledge_domains` ‚Üí what they need (e.g., ‚Äúenergy telemetry‚Äù, ‚Äúscheduler logs‚Äù)
  - `query_categories` ‚Üí what they ask (e.g., ‚Äúwhy job failed?‚Äù, ‚Äúdetect anomalies?‚Äù)
  - `data_sources` ‚Üí where data is located (Prometheus, IPMI, InfluxDB)
- This structure can directly populate:
  - ExaAgent persona registry (`personas.yaml`)
  - Query category table for the HPC chat system (`query_taxonomy.csv`)

---

**File:** `hpc_user_taxonomy.md`  
**Version:** 1.0 ‚Äî October 2025  
**Maintainer:** _ExaAgent Team_

---


---

## 3) Core Role Cards (compact)

Each card: **Responsibilities ‚Ä¢ Skills ‚Ä¢ Data/Tools ‚Ä¢ QCAT ‚Ä¢ Example queries**

### 3.1 Normal HPC Users (Scientists/Engineers)

* **Responsibilities**: Submit/monitor jobs, manage data, use preinstalled software, basic debugging, request quotas.
* **Skills**: Linux shell; schedulers (SLURM/PBS); modules; basic MPI/OpenMP; file systems; data transfer.
* **Data/Tools**: Job logs, stdout/err, project storage, environment modules; Singularity/Apptainer.
* **QCAT**: `JOB, SUP, DOCS, STOR, OBS`
* **Examples**: ‚ÄúWhy did my job fail?‚Äù ‚Ä¢ ‚ÄúWhich queue has the shortest wait?‚Äù ‚Ä¢ ‚ÄúHow to use Python with MPI?‚Äù

### 3.2 HPC Researchers (Performance/Energy/Anomaly)

* **Responsibilities**: Profile/benchmark; optimize performance; study energy; anomaly detection on multi‚Äëyear telemetry; power estimation from batch scripts; reproducible studies.
* **Skills**: MPI/OpenMP/CUDA/SYCL; HPCToolkit/TAU/VTune; Pandas/Dask/SQL; ML (Scikit‚Äëlearn/PyTorch); Kepler/RAPL; FAIR/MLflow/DVC.
* **Data/Tools**: Prometheus/InfluxDB; SLURM traces; logs; facility telemetry (PUE/cooling); parquet archives.
* **QCAT**: `PERF, ENERGY, AIOPS, OBS, CAP, DOCS`
* **Examples**: ‚ÄúDetect thermal anomalies in 2024 racks‚Äù ‚Ä¢ ‚ÄúEstimate job energy from this batch script‚Äù ‚Ä¢ ‚ÄúCompare configurations for best energy‚Äëperformance‚Äù.

### 3.3 System Administrators (SysAdmins/DevOps)

* **Responsibilities**: OS & middleware; scheduler config; upgrades/patching; user management; troubleshooting.
* **Skills**: Linux/Networking; SLURM/PBS; automation (Ansible/Helm/Terraform); containers; observability stacks.
* **Data/Tools**: Logs, node health, configs, images, CI/CD.
* **QCAT**: `OBS, SEC, JOB, STOR, NET, SUP`
* **Examples**: ‚ÄúWhy is node n45 drained?‚Äù ‚Ä¢ ‚ÄúUpdate CUDA driver for GPU partition‚Äù ‚Ä¢ ‚ÄúFair‚Äëshare limits for project X‚Äù.

### 3.4 Facility Admin / Technicians (Datacenter Ops)

* **Responsibilities**: Racks/power/cooling/networking; environmental monitoring; alarms; maintenance; sustainability.
* **Skills**: Electrical/mechanical; BMS/DCIM; IPMI/Redfish/SNMP/Modbus; safety/compliance.
* **Data/Tools**: IPMI sensors; BMS telemetry; maintenance schedules.
* **QCAT**: `FAC, ENERGY, CAP, OBS`
* **Examples**: ‚ÄúWhich racks show thermal anomalies?‚Äù ‚Ä¢ ‚ÄúPUE trend this quarter‚Äù ‚Ä¢ ‚ÄúAlarm history for CRAC‚Äë3‚Äù.

### 3.5 HPC System Designers / Architects

* **Responsibilities**: Plan HW/SW architecture; evaluate CPU/GPU/network/storage; cost‚Äëperformance; resilience; vendor liaison.
* **Skills**: Topologies (IB/Slingshot); storage design; cooling/power sizing; cost/perf modeling; benchmarks.
* **Data/Tools**: RFPs/specs; simulation models; benchmark results; capacity data.
* **QCAT**: `ARCH, CAP, ENERGY, NET, STOR, DOCS`
* **Examples**: ‚ÄúAMD vs Intel cost per TFLOP‚Äù ‚Ä¢ ‚ÄúCooling load estimate for 1 MW rack density‚Äù ‚Ä¢ ‚ÄúLatency impact of topology X‚Äù.

---

## 4) Extended Roles (optional but useful)

* **Monitoring Engineers / Observability** ‚Üí `OBS, AIOPS`
* **Storage Admins** ‚Üí `STOR, OBS`
* **Network Admins** ‚Üí `NET, OBS`
* **Security/IAM Officers** ‚Üí `SEC, OBS`
* **Support/Helpdesk** ‚Üí `SUP, DOCS, JOB`
* **Data Scientists on HPC** ‚Üí `JOB, PERF, STOR, DOCS`
* **Energy Managers** ‚Üí `ENERGY, FAC, CAP`
* **Project/Center Managers, PIs** ‚Üí `CAP, DOCS, ENERGY`
* **Vendors/Procurement** ‚Üí `ARCH, CAP`
* **Automation/Agentic Components (virtual users)** ‚Üí `AIOPS, OBS, ENERGY`

---

## 5) Role √ó Category Matrix (routing)

> ‚úÖ = frequent ‚Ä¢ ‚óá = occasional ‚Ä¢ ‚Äì = rarely/never

| Role ‚Üì / QCAT ‚Üí | JOB | PERF | ENERGY | OBS | STOR | NET | SEC | FAC | CAP | ARCH | SUP | AIOPS | DOCS |
| --------------- | :-: | :--: | :----: | :-: | :--: | :-: | :-: | :-: | :-: | :--: | :-: | :---: | :--: |
| Normal Users    |  ‚úÖ  |   ‚óá  |    ‚óá   |  ‚úÖ  |   ‚óá  |  ‚Äì  |  ‚Äì  |  ‚Äì  |  ‚Äì  |   ‚Äì  |  ‚úÖ  |   ‚Äì   |   ‚úÖ  |
| HPC Researchers |  ‚óá  |   ‚úÖ  |    ‚úÖ   |  ‚úÖ  |   ‚óá  |  ‚óá  |  ‚Äì  |  ‚óá  |  ‚úÖ  |   ‚óá  |  ‚Äì  |   ‚úÖ   |   ‚úÖ  |
| SysAdmins       |  ‚úÖ  |   ‚óá  |    ‚óá   |  ‚úÖ  |   ‚úÖ  |  ‚úÖ  |  ‚úÖ  |  ‚Äì  |  ‚óá  |   ‚Äì  |  ‚úÖ  |   ‚óá   |   ‚úÖ  |
| Facility Admins |  ‚Äì  |   ‚Äì  |    ‚úÖ   |  ‚úÖ  |   ‚Äì  |  ‚óá  |  ‚Äì  |  ‚úÖ  |  ‚úÖ  |   ‚Äì  |  ‚Äì  |   ‚óá   |   ‚úÖ  |
| Architects      |  ‚Äì  |   ‚óá  |    ‚úÖ   |  ‚óá  |   ‚úÖ  |  ‚úÖ  |  ‚Äì  |  ‚úÖ  |  ‚úÖ  |   ‚úÖ  |  ‚Äì  |   ‚óá   |   ‚úÖ  |

---

## 6) Persona Schema & Export Snippets

Use these snippets to seed configs.

### 6.1 YAML Persona (example: HPC Researchers)

```yaml
id: hpc_researcher
role: HPC Researchers
qcat: [PERF, ENERGY, AIOPS, OBS, CAP, DOCS]
responsibilities:
  - Benchmark, profile, and optimize applications
  - Anomaly detection on multi-year telemetry
  - Energy modeling and job-level power estimation
skills: [MPI, OpenMP, CUDA, SYCL, HPCToolkit, TAU, VTune, Pandas, SQL, MLflow, Kepler]
data_sources: [Prometheus, InfluxDB, SLURM-traces, Logs, FacilityTelemetry]
tools: [HPCToolkit, TAU, VTune, Grafana, MLflow, DVC]
example_queries:
  - Detect thermal anomalies in racks during Aug 2024
  - Estimate energy of this SLURM batch script
  - Compare configurations for best energy-performance
```

### 6.2 CSV Export (sample rows)

```csv
id,role,qcat,responsibilities,skills,data_sources,tools,examples
hpc_user,Normal HPC Users,"JOB|SUP|DOCS|STOR|OBS","Submit jobs; debug failures; manage data","Linux; SLURM; Modules","JobLogs; ProjectStorage","Apptainer; rsync","Why did my job fail?"
hpc_researcher,HPC Researchers,"PERF|ENERGY|AIOPS|OBS|CAP|DOCS","Profile & optimize; anomaly detection; energy modeling","MPI; CUDA; TAU; Pandas; Kepler","Prometheus; SLURM; FacilityTelemetry","HPCToolkit; Grafana; MLflow","Estimate energy of this batch script"
```

---

## 7) Example Queries by Category (grab‚Äëbag)

* **JOB**: Why pending? ‚Ä¢ Which queue shortest wait? ‚Ä¢ Increase GPU quota?
* **PERF**: MPI imbalance hotspots? ‚Ä¢ Roofline vs measured GFLOP/s ‚Ä¢ Node-to-node variability
* **ENERGY**: Node power cap at 250W? ‚Ä¢ PUE this month ‚Ä¢ Job J energy min/max/avg
* **OBS**: Top N nodes by CPU throttling ‚Ä¢ Alert history for exporter X ‚Ä¢ Dashboard link for partition Y
* **STOR**: Lustre OST with highest latency ‚Ä¢ Project Z quota ‚Ä¢ BeeGFS throughput anomaly
* **NET**: Fabric port errors ‚Ä¢ IB latency map ‚Ä¢ Oversubscription hotspots
* **SEC**: Failed logins by user ‚Ä¢ RBAC diff for group A ‚Ä¢ Expired tokens
* **FAC**: Rack R thermal map ‚Ä¢ CRAC-3 alarm history ‚Ä¢ Cooling efficiency by zone
* **CAP**: Predict 2026 GPU-hours ‚Ä¢ Utilization by project (monthly) ‚Ä¢ Headroom for new AI cluster
* **ARCH**: AMD vs Intel $/TFLOP ‚Ä¢ CXL benefits for workload W ‚Ä¢ Optimal topology for mixed AI+CFD
* **SUP**: Turn stdout/err into ticket ‚Ä¢ FAQ for OpenMP threads ‚Ä¢ Best practice for conda vs modules
* **AIOPS**: Train anomaly detector on 2022‚Äì2025 data ‚Ä¢ Autotune power caps ‚Ä¢ Explainable alert for power drift
* **DOCS**: Module usage for VASP ‚Ä¢ Policy for scratch purge ‚Ä¢ How to publish datasets FAIR

---

## 8) Governance & Guardrails

* Map QCAT ‚Üí **allowed backends** (e.g., JOB can query SLURM but not BMS control APIs).
* Attach **PII/secret** handling to SEC/DOCS routes.
* Keep **facility control actions** behind approvals; read‚Äëonly by default for FAC.

---

## 9) Change Log

* **v1**: Unified compact roles + global QCAT + routing matrix + export snippets.
