# ğŸ§  HPC <user_type>

## **Primary Mission**                            
- Bridge scientific computing with data-driven optimization â€” analyze, model, and improve performance, energy efficiency, reliability, and sustainability across the entire HPC system stack.

## **Core Tasks & Responsibilities**
- Develop new parallel algorithms or machine learning models for HPC workloads.
- Perform anomaly detection on multi-year cluster monitoring datasets (thermal, power, performance, logs).
- Analyze system logs, scheduler traces, and job metadata to identify performance bottlenecks and failure patterns.
- Conduct power consumption estimation from job scripts and runtime telemetry (batch script â†’ estimated Joules).
- Study long-term facility & IT data (cooling, temperature, power, humidity) to model correlations between compute load and energy usage.
- Perform operational data analysis for optimization and sustainability studies.

## **Key Studies / Knowledge Areas**
- HPC performance engineering (MPI, OpenMP, CUDA, SYCL)
- Anomaly detection & time-series analysis (ML/DL for logs & telemetry)
- Operational data analytics (Grafana/Prometheus data export, PostgreSQL, InfluxDB, parquet data)
- Energy modeling & prediction (Kepler, RAPL, PowerAPI, pod-level prediction models)
- Cooling systems & facility telemetry interpretation (BMS/DCIM data integration)
- Data wrangling and visualization (Pandas, PySpark, DVC, MLflow)
- Scientific reproducibility (experiment tracking, model versioning, FAIR datasets)
- HPC workload characterization (job patterns, resource usage clusters, performance variability)

## **Data Sources They Analyze**
- Monitoring data: node-level metrics (CPU/GPU utilization, power, temperature, fan speed)
- Logs: job scheduler logs, syslogs, application logs
- Batch scripts: SLURM/PBS job submission scripts (for workload classification & power estimation)
- Databases: Prometheus, PostgreSQL, InfluxDB, parquet archives
- Facility telemetry: PUE, cooling power, rack temperature distributions

## **Tools & Frameworks**
- HPC profiling tools (HPCToolkit, TAU, Score-P, VTune)
- Data analytics frameworks (Pandas, Dask, PySpark, SQL)
- Visualization (Grafana, Plotly, Matplotlib)
- ML/DL frameworks (Scikit-Learn, PyTorch, TensorFlow)
- Energy monitoring (Kepler, RAPL, IPMI)
- Cluster telemetry ingestion (Prometheus, ExaData, InfluxDB)
- Reproducibility stacks (MLflow, DVC, GitLab CI/CD)

## **Research Themes / Study Topics**
- System-wide anomaly detection (thermal, power, network)
- Predictive maintenance from multi-year logs
- Energy-aware job scheduling and node management
- Workload characterization and job class detection
- Cross-correlation of IT + facility data (server vs cooling)
- Data-driven optimization of cooling & compute efficiency
- Multi-modal telemetry fusion (metrics + logs + job scripts)


## **Example Queries for Multi-Agentic Chat System**

| # | Code | Category | Description | Example Queries | Priority |
|---|------|-----------|--------------|-----------------|-----------|
| 1 | **JOB** | **Job & Workflow Management** | Submitting, managing, and debugging jobs; handling queues and workflows. | â€œSubmit my SLURM job script.â€<br>â€œShow estimated queue wait time for GPU partition.â€<br>â€œCancel job 10234.â€<br>â€œList my failed jobs this week.â€<br>â€œRe-run job 4321 with longer walltime.â€ | **H** |
| 2 | **PERF** | **Performance & Optimization** | Profiling and optimizing application and system performance. | â€œProfile my job using nvprof.â€<br>â€œWas my job memory-bound?â€<br>â€œIdentify MPI communication bottlenecks.â€<br>â€œCompare runtime of exp_12 vs exp_15.â€<br>â€œSuggest compiler flags for vectorization.â€ | **H** |
| 3 | **DATA** | **Data & Storage Management** | Managing data, I/O performance, and storage utilization. | â€œCopy results from scratch to project folder.â€<br>â€œCheck Lustre I/O bandwidth.â€<br>â€œList large files in /scratch.â€<br>â€œHow much project storage is left?â€ | **M** |
| 4 | **MON** | **Monitoring & Observability** | Collecting and analyzing metrics, logs, and telemetry across nodes, jobs, and racks. | â€œShow CPU/GPU utilization for job 23456.â€<br>â€œPlot my last weekâ€™s node usage.â€<br>â€œDetect anomalies in node power consumption over the last 3 years.â€<br>â€œShow temporal drift in node utilization distribution.â€ | **H** |
| 5 | **ENERGY** | **Power, Energy & Sustainability** | Monitoring energy usage, estimating job power cost, and analyzing sustainability KPIs. | â€œEstimate power cost of this batch script.â€<br>â€œWhich jobs caused thermal stress in 2023.â€<br>â€œCorrelate cooling energy and compute load during summer 2024.â€<br>â€œGenerate energy efficiency report for my project.â€<br>â€œShow thermal map of GPU nodes.â€ | **H** |
| 6 | **FAC** | **Facility, Infrastructure & Environmental Systems** | Managing racks, CRAC units, alarms, and environmental sensors (temperature, humidity, PUE). | â€œWhich racks show thermal anomalies?â€<br>â€œAlarm history for CRAC unit 3.â€<br>â€œShow cooling efficiency by zone.â€<br>â€œCurrent PUE trend for datacenter.â€ | **L** |
| 7 | **SEC** | **Security, Access & Policy Management** | Handling IAM, quota limits, authentication, and compliance queries. | â€œWhy was my SSH access denied?â€<br>â€œShow my remaining quota.â€<br>â€œWhich users can access dataset X?â€ | **L** |
| 8 | **ARCH** | **System Architecture, Design & Capacity Planning** | Understanding and planning HPC system design, capacity, and scalability. | â€œWhich nodes have A100 GPUs?â€<br>â€œShow topology of interconnect fabric.â€<br>â€œForecast storage capacity for next year.â€<br>â€œCompare rack power density trends.â€ | **M** |
| 9 | **AIOPS** | **AI, Automation & Intelligent Operations** | ML/AI-driven analytics for anomaly detection, predictive maintenance, and optimization. | â€œDetect cooling anomalies using ML models.â€<br>â€œPredict node failure probability.â€<br>â€œGenerate feature dataset for model training from Prometheus + SLURM logs.â€<br>â€œRecommend energy-aware job scheduling.â€ | **H** |
| 10 | **DOCS** | **Documentation, Support & Knowledge Assistance** | Providing knowledge, tutorials, troubleshooting, and HPC best practices. | â€œHow to run TensorFlow with SLURM?â€<br>â€œExplain difference between partitions.â€<br>â€œShow documentation for Kepler energy plugin.â€ | **M** |

## Priority Tags Summary

| Priority | Meaning | Categories |
|----------|---------|------------|
| **H (High)** | Frequent, critical for research workflows | Job Submission, Resource Discovery, Monitoring, Debugging, Profiling, Data Management |
| **M (Medium)** | Periodic needs | Environment Setup, Reproducibility, Documentation, Visualization |
| **L (Low)** | Rare but high-value | Facility Awareness, Security, Collaboration |

---

