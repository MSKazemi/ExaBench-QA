# ğŸ—ï¸ HPC & Monitoring System User Taxonomy

This document defines a **complete taxonomy of all user types** that interact with an **HPC system** and its **monitoring ecosystem** (IT + facility).  
It includes scientific users, operations teams, facility managers, researchers, and emerging AI-driven agents â€” suitable for use in RAG models, chat systems, or knowledge graphs such as **ExaAgent**.

---

## ğŸ§‘â€ğŸ”¬ 1. Scientific & Application Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Normal HPC Users (Scientists / Engineers)** | Submit and monitor jobs, manage data, use pre-installed scientific software. | SLURM/PBS, job logs, project storage, modules, Singularity containers. | â€œWhy did my job fail?â€<br>â€œHow to request more GPU nodes?â€ |
| **HPC Researchers (Performance, Energy, Anomaly Studies)** | Analyze performance and energy efficiency; perform anomaly detection and long-term monitoring studies; estimate power from job scripts; optimize workloads and scheduling. | Monitoring DBs, Prometheus, InfluxDB, Kepler, logs, SLURM traces, facility data, DVC datasets. | â€œDetect anomalies in 3-year power data.â€<br>â€œEstimate energy of this batch script.â€ |
| **Data Scientists / AI Researchers on HPC** | Use HPC for ML/AI workloads; study scalability and data handling; integrate with model registries. | PyTorch, TensorFlow, MLflow, DVC, JupyterHub. | â€œTrain transformer model on GPU nodes.â€ |
| **Computational Domain Scientists** | Domain-specific workloads (CFD, MD, weather, etc.) using tuned software. | OpenFOAM, GROMACS, VASP, WRF, LAMMPS. | â€œHow to enable OpenMP in VASP job?â€ |
| **Industrial / Enterprise Users** | External customers accessing HPC through portals or APIs. | Web portals, REST APIs, workflow managers. | â€œSubmit workflow through API.â€ |

---

## ğŸ§° 2. Infrastructure & Operations Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **System Administrators (SysAdmins / DevOps)** | Maintain OS, scheduler, user accounts, and reliability of nodes. | Linux, SLURM, Prometheus, Ansible, Helm, Kubernetes, logs. | â€œWhy is node n45 offline?â€ |
| **Monitoring Engineers / Observability Specialists** | Manage metric collection, alerting, and dashboarding. | Prometheus, Grafana, Loki, Alertmanager, Kepler. | â€œCPU utilization trend per partition.â€ |
| **Storage Administrators** | Manage parallel file systems and backups. | Lustre, BeeGFS, Ceph, GPFS, object storage telemetry. | â€œCheck Lustre throughput anomalies.â€ |
| **Network Administrators** | Monitor and maintain interconnects. | SNMP, Infiniband, Slingshot, Grafana dashboards. | â€œLatency statistics for fabric ports.â€ |
| **Security Officers / IAM Managers** | Oversee authentication, RBAC, and compliance. | Keycloak, LDAP, IAM logs, audit trails. | â€œList failed login attempts.â€ |
| **Support Engineers / Helpdesk Staff** | Resolve user issues, collect logs, maintain FAQs. | Ticketing systems, job reports. | â€œUser job exceeded time limit.â€ |
| **Software Stack Maintainers** | Build and containerize HPC toolchains. | Spack, EasyBuild, CI/CD pipelines. | â€œRebuild GCC 13 toolchain with CUDA.â€ |

---

## ğŸ­ 3. Facility & Hardware Operations Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Facility Administrators / Technicians** | Manage racks, cooling, power, sensors, and safety. | IPMI, Redfish, BMS/DCIM, Modbus, SNMP. | â€œReport temperature map per rack.â€ |
| **Energy Managers / Sustainability Analysts** | Analyze PUE, energy efficiency, and sustainability. | Power meters, facility telemetry, PUE logs. | â€œCompute PUE for past quarter.â€ |
| **Hardware Engineers / Maintenance Staff** | Diagnose and replace faulty components. | IPMI logs, service tickets. | â€œWhich nodes reported fan failure?â€ |
| **Procurement & Vendor Liaisons** | Manage warranties and evaluate new hardware. | RFPs, inventory DBs. | â€œTrack warranty status for GPU nodes.â€ |

---

## ğŸ§± 4. Architecture, Design & Strategic Planning Users

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **HPC System Architects / Designers** | Design and size new clusters; define topology and storage. | RFPs, simulation models, benchmark data. | â€œEvaluate AMD vs Intel node cost/TFlop.â€ |
| **Capacity Planners** | Forecast usage, hardware refreshes, and expansion needs. | Historical metrics, queue statistics. | â€œPredict CPU core demand in 2026.â€ |
| **Performance Modelers** | Build performance and cost trade-off models. | Telemetry archives, simulators. | â€œSimulate scalability for 1k-node workload.â€ |
| **Policy & Project Managers** | Plan budgets, define priorities, align R&D goals. | KPI dashboards, utilization reports. | â€œMonthly utilization summary by project.â€ |

---

## ğŸ“Š 5. Data Analytics & Observability Researchers (Cross-Layer)

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Operational Data Scientists** | Analyze long-term telemetry for optimization and sustainability. | SQL, Pandas, PySpark, parquet datasets. | â€œCorrelate cooling power and CPU load.â€ |
| **Anomaly Detection Researchers** | Use AI/ML to detect abnormal thermal/power/network patterns. | Kepler, ThermADNet, HazardNet, Grafana alerts. | â€œFind nodes with thermal drift.â€ |
| **Digital Twin Developers** | Create virtual replicas of HPC behavior for predictive control. | InfluxDB, ML models, simulation tools. | â€œTrain digital twin on 2022-2025 telemetry.â€ |
| **AIOps / MLOps Engineers** | Build autonomous pipelines for anomaly detection and optimization. | MLflow, DVC, FastAPI, LangGraph, Prometheus APIs. | â€œDeploy model to optimize node power caps.â€ |

---

## ğŸ§© 6. Management, Policy, and External Stakeholders

| **Sub-Role** | **Description / Objectives** | **Typical Data / Tools Used** | **Example Queries** |
|---------------|------------------------------|-------------------------------|---------------------|
| **Center Directors / Project PIs** | Oversee operations, KPIs, and collaborations. | KPI dashboards, energy reports. | â€œShow overall energy savings since 2023.â€ |
| **Funding Agencies / Auditors** | Evaluate efficiency, utilization, and sustainability. | Reports, compliance datasets. | â€œGenerate annual energy audit report.â€ |
| **External Collaborators (EU Partners / Vendors)** | Access shared telemetry or datasets for research. | Shared dashboards, APIs, data exports. | â€œRequest metrics subset for WP3 analysis.â€ |

---

## ğŸ¤– 7. Automation & Agentic Components (Emerging Virtual Users)

In AIOps-enabled HPC frameworks (e.g., **ExaAgent**, **KubeIntellect**), automated agents act as *virtual users* interacting with monitoring and control layers.

| **Agent Type** | **Purpose / Function** | **Interacts With** |
|----------------|------------------------|--------------------|
| **Monitoring Agent (ExaSage / Kepler)** | Collects metrics and logs across nodes and facilities. | Prometheus, IPMI, InfluxDB. |
| **Anomaly Detection Agent (ThermADNet / HazardNet)** | Detects and explains abnormal system behavior. | Telemetry DBs, alert systems. |
| **AI Scheduler Agent** | Dynamically optimizes job scheduling and resource allocation. | SLURM APIs, Prometheus metrics. |
| **Chat / RAG Agent (User Assistant)** | Provides conversational access to data, documentation, and monitoring systems. | Vector DB, documentation corpus, APIs. |

---

## ğŸŒ 8. Hierarchical Summary (Mind-Map)

```plaintext
HPC & Monitoring Users
â”œâ”€â”€ Scientific Users
â”‚ â”œâ”€â”€ Normal Users
â”‚ â”œâ”€â”€ HPC Researchers
â”‚ â”œâ”€â”€ AI/ML Researchers
â”‚ â”œâ”€â”€ Domain Scientists
â”‚ â””â”€â”€ Industrial Users
â”œâ”€â”€ Infrastructure & Operations
â”‚ â”œâ”€â”€ SysAdmins
â”‚ â”œâ”€â”€ Monitoring Engineers
â”‚ â”œâ”€â”€ Storage / Network Admins
â”‚ â”œâ”€â”€ Security Officers
â”‚ â””â”€â”€ Support Staff
â”œâ”€â”€ Facility & Hardware
â”‚ â”œâ”€â”€ Facility Technicians
â”‚ â”œâ”€â”€ Energy Managers
â”‚ â”œâ”€â”€ Hardware Engineers
â”‚ â””â”€â”€ Vendors / Procurement
â”œâ”€â”€ Architecture & Planning
â”‚ â”œâ”€â”€ System Architects
â”‚ â”œâ”€â”€ Capacity Planners
â”‚ â”œâ”€â”€ Policy Managers
â”‚ â””â”€â”€ Project Directors
â”œâ”€â”€ Data Analytics & Research
â”‚ â”œâ”€â”€ Operational Data Scientists
â”‚ â”œâ”€â”€ Anomaly Detection Researchers
â”‚ â”œâ”€â”€ Digital Twin Developers
â”‚ â””â”€â”€ AIOps Engineers
â”œâ”€â”€ Management & External
â”‚ â”œâ”€â”€ Center Directors
â”‚ â”œâ”€â”€ Funding Agencies
â”‚ â””â”€â”€ External Collaborators
â””â”€â”€ Automation Agents
â”œâ”€â”€ Monitoring Agent
â”œâ”€â”€ Anomaly Detection Agent
â”œâ”€â”€ AI Scheduler Agent
â””â”€â”€ Chat / RAG Agent
```


---

## ğŸ§© Notes for RAG / Chat Integration

- Each **user type** can be mapped to:
  - `knowledge_domains` â†’ what they need (e.g., â€œenergy telemetryâ€, â€œscheduler logsâ€)
  - `query_categories` â†’ what they ask (e.g., â€œwhy job failed?â€, â€œdetect anomalies?â€)
  - `data_sources` â†’ where data is located (Prometheus, IPMI, InfluxDB)
- This structure can directly populate:
  - ExaAgent persona registry (`personas.yaml`)
  - Query category table for the HPC chat system (`query_taxonomy.csv`)

---

**File:** `hpc_user_taxonomy.md`  
**Version:** 1.0 â€” October 2025  
**Maintainer:** _ExaAgent Team_

---
