
# HPC User Personas & Capability Matrix

| User Type                               | Primary Mission                                                  | Top Responsibilities (condensed)                                                                                                                           | Key Data Sources                                                                                                                             | Typical Tools                                                                                                                  | High-Priority Categories (Codes)                                                                                                               | Example High-Priority Query                                        |
| --------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **Facility Admin / Technicians**        | Keep facility power/cooling safe, reliable, and energy-efficient | Monitor env. (temp/humidity/airflow); operate CRAC/RDHX/chillers; track PUE/energy; handle alarms; plan maintenance; manage assets; enforce safety         | BMS/DCIM; IPMI/Redfish; PDU/UPS/CRAC meters; facility logs; safety docs; switch env metrics; PUE/CO₂ reports                                 | Prometheus/Grafana; BMS/DCIM (SNMP/Redfish); Python automations; Energy analytics; RAG for SOPs                                | **H:** MON, COOL, ENERGY, HEALTH, ALARM, IRCA • **M:** MAINT, SUSTAIN, ASSET, NET, DOCS, TREND • **L:** INT, SAFE, CTRL                        | “List all active **critical alarms** and the affected racks.”      |
| **Normal Users (Scientists/Engineers)** | Run jobs efficiently; manage data; follow best practices         | Submit/monitor jobs; manage data/quota; set up modules/envs; debug failures; basic perf checks; use GPUs; collaborate                                      | Scheduler acct/logs; job stdout/err; CPU/GPU/mem telemetry; storage/quota stats; status dashboards                                           | SLURM/PBS/LSF; Modules/Conda; Jupyter; scp/rsync/Globus; Profilers; RAG docs; ExaSage views                                    | **H:** JOB, QUEUE, DATA, SOFT, DEBUG, DOCS • **M:** PERF, GPU, STOR, INTER, COLL, MON • **L:** AUTO, SEC, ENERGY                               | “**Submit** my script with 4 GPUs and 64 GB RAM.”                  |
| **System Administrators**               | Ensure reliability, performance, security of clusters            | Manage users/RBAC; maintain nodes/GPUs/network/storage; control queues/partitions; patch/firmware; monitor/alert; security/compliance; backups; automation | Node/job metrics; scheduler logs; syslog/dmesg; FS telemetry (OST/MDT); facility IPMI/BMS; auth/security logs; quota DBs                     | SLURM/PBS/LSF; Prometheus/Grafana/ELK; IPMI/Redfish/SNMP; Ansible/Terraform; Spack/containers; Lustre/BeeGFS/Ceph; Python/Bash | **H:** JOB, RES, USER, MON, PERF, STOR, SEC, SWENV, NODE, AUTO, LOGS, SUPPORT • **M:** NET, ENERGY, FAC, DOCS, LIC • **L:** BACKUP, API, AIOPS | “**Drain** node n101 and migrate pending GPU jobs.”                |
| **System Designers / Architects**       | Architect scalable, efficient, reliable systems                  | Define node mix/topology; size CPU/GPU/mem; plan fabric; design storage; run benchmarks; set telemetry/alerts; CAPEX/OPEX/TCO; resilience & standards      | Telemetry (IPMI/GPU/CPU); network counters/IB errors; facility power/cooling maps; storage/I/O stats; HPL/STREAM results; cost/warranty data | ExaSage/Facility agents; SLURM + benchmarks (HPL/STREAM); topology & storage tools; RAG for standards                          | **H:** PERF, MON, ENERGY, ARCH • **M:** JOB, DATA, FAC, SEC, AIOPS, DOCS                                                                       | “Estimate **LINPACK scaling** for 512 nodes and flag bottlenecks.” |
| **HPC Researchers / Analysts**          | Data-driven optimization of performance, energy, reliability     | Build ML/analytics; anomaly detection; analyze logs/traces; estimate job energy; correlate IT vs facility; reproducible studies                            | Prometheus/Influx/Postgres; parquet exports; scheduler traces; syslogs/app logs; Kepler/RAPL; facility PUE/cooling                           | Pandas/Dask/PySpark/SQL; ML (Sklearn/PyTorch/TF); HPCToolkit/TAU; Grafana/Plotly; MLflow/DVC; Kepler/IPMI                      | **H:** JOB, PERF, MON, ENERGY, AIOPS • **M:** DATA, ARCH, DOCS • **L:** SEC, FAC                                                               | “**Detect** power anomalies over 3 years and rank culprit jobs.”   |
