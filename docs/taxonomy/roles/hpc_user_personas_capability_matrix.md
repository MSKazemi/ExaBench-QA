
# HPC User Personas & Capability Matrix

| User Type                                 | Primary Mission                                                                                   | Top Responsibilities (condensed)                                                                                             | Key Data Sources                                                                                  | Typical Tools                                                                        | **High-Priority Categories (Codes)** | Example High-Priority Query                                            |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ------------------------------------ | ---------------------------------------------------------------------- |
| **Facility Admin / Technicians**          | Ensure safe, reliable, and energy-efficient operation of HPC datacenter power and cooling systems | Monitor environmental metrics; manage CRAC/RDHX/chillers; track energy/PUE; handle alarms; maintenance and safety compliance | BMS/DCIM telemetry, IPMI/Redfish, PDU/UPS/CRAC meters, facility logs, CO₂ reports                 | Prometheus/Grafana, BMS/DCIM tools (SNMP/Redfish), Python automation, RAG for SOPs   | **MON, ENERGY, FAC, AIOPS, DOCS**    | “List all **active critical alarms** and show affected racks.”         |
| **Normal Users (Scientists / Engineers)** | Run computational workloads efficiently and manage research data                                  | Submit & monitor jobs, manage data, configure environments, debug failures, optimize performance                             | Scheduler logs, job outputs, CPU/GPU telemetry, storage usage                                     | SLURM/PBS, Modules/Conda, JupyterHub, Profilers, RAG Docs                            | **JOB, DATA, PERF, MON, DOCS**       | “**Submit** my SLURM job with 4 GPUs and 64 GB RAM.”                   |
| **System Administrators**                 | Maintain reliability, performance, and security of HPC clusters                                   | Manage users/RBAC, monitor nodes, control queues, maintain OS/firmware, ensure compliance, automate tasks                    | Node/job metrics, scheduler logs, syslogs, storage telemetry, IPMI/BMS data, security logs        | SLURM, Prometheus/Grafana/ELK, IPMI/Redfish, Ansible/Terraform, Spack, Lustre/BeeGFS | **JOB, MON, DATA, SEC, AIOPS**       | “**Drain** node n101 and requeue pending jobs.”                        |
| **System Designers / Architects**         | Architect scalable, efficient HPC systems across compute, storage, and facility layers            | Define node mix and topology; plan interconnects and storage; run benchmarks; analyze efficiency and cost                    | IPMI/GPU/CPU telemetry, IB counters, PUE maps, benchmark results, procurement data                | ExaSage, Benchmark suites (HPL/STREAM), Topology tools, RAG for standards            | **ARCH, PERF, ENERGY, MON, DOCS**    | “Estimate **LINPACK scaling** for 512 nodes and identify bottlenecks.” |
| **HPC Researchers / Analysts**            | Analyze and model system performance, energy, and reliability using data-driven methods           | Build ML models for anomalies; analyze logs & telemetry; estimate energy; correlate compute vs facility data                 | Prometheus/InfluxDB/PostgreSQL, parquet exports, SLURM logs, Kepler/RAPL data, facility telemetry | Pandas/Dask, PyTorch/TensorFlow, HPCToolkit, Grafana, MLflow/DVC                     | **AIOPS, PERF, ENERGY, DATA, MON**   | “**Detect** power anomalies across 3 years and rank top outlier jobs.” |
